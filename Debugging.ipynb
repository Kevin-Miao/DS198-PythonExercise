{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "democratic-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gzip\n",
    "import json\n",
    "from nltk.corpus import words\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-newark",
   "metadata": {},
   "source": [
    "# DS198-003: Tech Foundations - Debugging\n",
    "\n",
    "In this demo, we will find useful ways to debug through code using Python and Jupyter Notebooks! This demo has been created by [Kevin Miao](mailto:kevinmiao@cs.berkeley.edu) for UC Berkeley's DS198-003 (Spring '22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-deployment",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fe50ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-14 21:18:37--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\n",
      "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
      "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2460495 (2.3M) [application/x-gzip]\n",
      "Saving to: ‘reviews_Musical_Instruments_5.json.gz.1’\n",
      "\n",
      "reviews_Musical_Ins 100%[===================>]   2.35M  2.18MB/s    in 1.1s    \n",
      "\n",
      "2022-03-14 21:18:38 (2.18 MB/s) - ‘reviews_Musical_Instruments_5.json.gz.1’ saved [2460495/2460495]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\n",
    "    \n",
    "with gzip.open(\"reviews_Musical_Instruments_5.json.gz\", \"rb\") as f:\n",
    "    data = [json.loads(line) for line in f]  # json.loads(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a7a66",
   "metadata": {},
   "source": [
    "Here we are loading in an Amazon review from a the dataset above. An example is embedded below. We have the overall score, a summary, the review time and the review text, as well as the name of the reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "236ea151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewerID': 'A2IBPI20UZIR0U',\n",
       " 'asin': '1384719342',\n",
       " 'reviewerName': 'cassandra tu \"Yeah, well, that\\'s just like, u...',\n",
       " 'helpful': [0, 0],\n",
       " 'reviewText': \"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\",\n",
       " 'overall': 5.0,\n",
       " 'summary': 'good',\n",
       " 'unixReviewTime': 1393545600,\n",
       " 'reviewTime': '02 28, 2014'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a489477",
   "metadata": {},
   "source": [
    "We are also loading in a **dictionary** with all english words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "341a534c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'Aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'Aaron']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = words.words()\n",
    "word_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f0abe",
   "metadata": {},
   "source": [
    "In **natural language processing**, we match all the words of a text to an index in a dictionary that contains all words. For instance, 'Hello world' would become [# index of `hello`, `# index of world`]. Let's write a function that does that! Also, here below I provided a list of punctuation marks that might come in handy later. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acc9abc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-calculator",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdba4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'I', 'love', 'data', 'science']\n",
      "hello - 83713\n",
      "I - 90447\n",
      "love - 108498\n",
      "data - 48777\n",
      "science - 175096\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"hello I love data science\"\n",
    "split_sentence = sample_sentence.split()\n",
    "print(split_sentence)\n",
    "tokens = []\n",
    "for i, word in enumerate(split_sentence):\n",
    "    token = word_list.index(word)\n",
    "    tokens.append(token)\n",
    "    print(f\"{word} - {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3028283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence: Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Code failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(sentence, dictionary)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(split_sentence):\n\u001b[0;32m---> 13\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mword_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "\u001b[0;31mValueError\u001b[0m: 'Not' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m sample_sentence \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample Sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m tokenization \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(sentence, dictionary)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCode failed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Code failed"
     ]
    }
   ],
   "source": [
    "def tokenize(sentence, dictionary):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        - sentence (str): a sentence of words\n",
    "        - dictionary List[str]: a list of all English words\n",
    "    Returns:\n",
    "        - tokenization List[int]: a list of all indices of the dictionary for the words in our sentence\n",
    "    \"\"\"\n",
    "    split_sentence = sentence.split()\n",
    "    tokens = []\n",
    "    try:\n",
    "        for i, word in enumerate(split_sentence):\n",
    "            token = word_list.index(word)\n",
    "            tokens.append(token)\n",
    "            print(f\"{word} - {token}\")\n",
    "        return tokens\n",
    "    except:\n",
    "        raise Exception('Code failed')\n",
    "\n",
    "sample_sentence = data[0]['reviewText']\n",
    "print(f\"Example Sentence: {sample_sentence}\")\n",
    "tokenization = tokenize(sample_sentence, word_list)\n",
    "print(f\"Tokenization: {tokenization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-majority",
   "metadata": {},
   "source": [
    "### Print Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-broad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intimate-berlin",
   "metadata": {},
   "source": [
    "### Debugger `pdb`\n",
    "\n",
    "The basics:\n",
    "- `pdb.set_trace()`: Set a breakpoint\n",
    "- `n`: To the next line\n",
    "- `r`: Run code\n",
    "- `e`: Exit debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "mexican-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-personal",
   "metadata": {},
   "source": [
    "## Type Hinting\n",
    "\n",
    "Write the `map` function. The `MapFilter` function takes in a list or an array, a tuple that indicates the range of indices that needs to be returned, and a function that needs to be applied on this subsequence of the iterable. It returns a final number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "million-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = [1.7,4,56.33,7.2,7.0,8,9.12,10,-2, 5, 7]\n",
    "input_range = (3, 6)\n",
    "input_function = lambda x: abs(x**2 + x*2)\n",
    "\n",
    "###### START #######\n",
    "\n",
    "def map():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "###### END ########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
